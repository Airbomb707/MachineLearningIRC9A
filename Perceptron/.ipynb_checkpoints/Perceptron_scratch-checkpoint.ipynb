{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012bc074-b159-4b45-9d55-8ea56b729340",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f0945-2386-4a48-b45f-e5a223137223",
   "metadata": {},
   "source": [
    "#### Sonar dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f92bf-e456-472d-98f3-e3734fd907c4",
   "metadata": {},
   "source": [
    "The file “sonar.mines” contains 111 patterns obtained by bouncing sonar signals off a metal cylinder at various angles and under various conditions. The file “sonar.rocks” contains 97 patterns obtained from rocks under similar conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock.\n",
    "\n",
    "Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.\n",
    "\n",
    "The label associated with each record contains the letter “Rock” if the object is a rock and “Mine” if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17434381-50e0-420d-a1db-23b6c042e7ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d62659b-966a-4e16-9c6d-526db02c8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron Algorithm on the Sonar Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a477d212-be2e-4c41-8456-66341a538bbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663b3d88-8361-4398-a0ea-bf3705cc9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Exclude the first row\n",
    "def load_csv_exclude_first_row(filename):\n",
    "    dataset = load_csv(filename)\n",
    "    return dataset[1:]  # Exclude the first row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255f9f0-e40b-4ae4-bfe9-80e7aaa46798",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b517fe-5239-4444-80d4-c139cdaad98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6944418-b6ba-4898-bf55-151c2791da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb84ae3-705c-459e-9c1f-e2689f011088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "371dcf5f-cc30-4bc2-902a-4587ce8f75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1afad02e-2c9d-40c0-b970-3689cde0ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross-validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d65021b-b410-4ad8-8d92-81c8bf16e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row) - 1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return 1.0 if activation >= 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f55288-dcd4-4d2e-a26a-a22251a58a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "    weights = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            prediction = predict(row, weights)\n",
    "            error = row[-1] - prediction\n",
    "            weights[0] = weights[0] + l_rate * error\n",
    "            for i in range(len(row) - 1):\n",
    "                weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44b5065e-6e77-4231-acb4-f5066a4d91fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron Algorithm With Stochastic Gradient Descent\n",
    "def perceptron(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    weights = train_weights(train, l_rate, n_epoch)\n",
    "    for row in test:\n",
    "        prediction = predict(row, weights)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c540d199-e51e-4899-af54-2048e32d06c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [76.81159420289855, 69.56521739130434, 72.46376811594203]\n",
      "Mean Accuracy: 72.947%\n"
     ]
    }
   ],
   "source": [
    "# Test the Perceptron algorithm on the sonar dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = './datasets/sonar.csv'\n",
    "dataset = load_csv_exclude_first_row(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# convert string class to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 3\n",
    "l_rate = 0.01\n",
    "n_epoch = 500\n",
    "scores = evaluate_algorithm(dataset, perceptron, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d94526-0390-4d43-9299-862b018ff92d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
