{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012bc074-b159-4b45-9d55-8ea56b729340",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5beedc-8cf9-4c1e-aa17-d75ad0a59fe4",
   "metadata": {},
   "source": [
    "A **perceptron** is a classification model that consists of a set of weights and scores for every feature, and a threshold. The perceptron multiplies each weight by its corresponding score, and adds them. If this score is greater than or equal to the threshold, then the perceptron returns **True**, or 1. If it is smaller, then it returns **False**, or the value 0.\n",
    "\n",
    "The perceptron is a linear classifier and is limited to solving linearly separable problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7edd86-4491-4074-93db-64cefbba7140",
   "metadata": {},
   "source": [
    "#### Pseudocode"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2a1b291-3820-446b-997a-da2be66245db",
   "metadata": {},
   "source": [
    "Input:\n",
    "    A dataset of points, where every point has a positive or negative label.\n",
    "    A number of epochs, n.\n",
    "    A learning rate.\n",
    "\n",
    "Procedure:\n",
    "Initialize weights and bias to small random values, and set the learning rate and maximum number of iterations.\n",
    "\n",
    "For each training example (X, y) in the training dataset:\n",
    "    1. Calculate the predicted output using the current weights and bias\n",
    "    2. Compute the error as the the difference between the true label and the predicted label\n",
    "    3. Update the weights and bias using the error and the learning rate\n",
    "\n",
    "Repeat for n number of iterations or until convergence (when errors are sufficiently small).\n",
    "\n",
    "The perceptron will predict 1 or 0 based on the sign of the predicted outputs\n",
    "    If prediction >= 0, predict 1\n",
    "    If prediction < 0, predict 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f0945-2386-4a48-b45f-e5a223137223",
   "metadata": {},
   "source": [
    "#### Sonar dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f92bf-e456-472d-98f3-e3734fd907c4",
   "metadata": {},
   "source": [
    "The file contains various patterns obtained by bouncing sonar signals off a metal cylinder or from rocks at various angles and under various conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock.\n",
    "\n",
    "Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.\n",
    "\n",
    "The label associated with each record contains the letter “Rock” if the object is a rock and “Mine” if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062873e-3708-4329-a69d-8849c9969c49",
   "metadata": {},
   "source": [
    "## Perceptron from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17434381-50e0-420d-a1db-23b6c042e7ee",
   "metadata": {},
   "source": [
    "We import simple packages for randomizing a seed, and the CSV reader class for reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d62659b-966a-4e16-9c6d-526db02c8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron Algorithm on the Sonar Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafbf9c7-c142-40e4-b6ac-d8186cdfec54",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a477d212-be2e-4c41-8456-66341a538bbc",
   "metadata": {},
   "source": [
    "We define our loading function, which takes a string representing the name of the CSV file as an argument. We then initialize an empty list inside the function to store the data from the CSV file. We open the file in read mode ('r'), create a csv_reader object and pass our file, then iterate through each row in the CSV file. If the row is empty, continue to the next row.\n",
    "\n",
    "Afterwards we define a function to load the dataset using our previous function, skip the first row (header), and return all the remaining rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663b3d88-8361-4398-a0ea-bf3705cc9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Exclude the first row\n",
    "def load_csv_exclude_first_row(filename):\n",
    "    dataset = load_csv(filename)\n",
    "    return dataset[1:]  # Exclude the first row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255f9f0-e40b-4ae4-bfe9-80e7aaa46798",
   "metadata": {},
   "source": [
    "Here we take a specified column for the current row, then strip any extra whitespace using strip(), and convert the string to float to make sure we are working with numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b517fe-5239-4444-80d4-c139cdaad98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68252411-5acd-461a-ade8-3cffd8d4abc5",
   "metadata": {},
   "source": [
    "We iterate through the dataset and create a list which contains all the values from the target column. We then create a set called ***unique*** to store the unique values from the *class_values* list, to ensure that each unique value is considered only once. We initialize an empty dictionary called lookup. This dictionary will be used to map each unique string value to a unique integer. The key will be the string value, and the value will be the corresponding integer.\n",
    "\n",
    "Finally, return the lookup dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6944418-b6ba-4898-bf55-151c2791da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values) # Store unique values\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b72fe59-20c4-46f2-b2c0-9d5065846f82",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926ee5e-0741-4076-822f-d890f6c9bc9b",
   "metadata": {},
   "source": [
    "K-fold cross-validation is a technique for evaluating predictive models. The dataset is divided into k subsets or folds. The model is trained and evaluated k times, using a different fold as the validation set each time. Performance metrics from each fold are averaged to estimate the model's generalization performance.\n",
    "\n",
    "For each fold, we iterate n times. Within each iteration, we create a new fold and populate it with fold_size samples randomly selected from the dataset copy.\n",
    "\n",
    "We use randrange to select a random index from dataset_copy, and add it to the fold. We then remove it from dataset_copy to avoid duplication.\n",
    "\n",
    "Finally, the fold is added to dataset_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb84ae3-705c-459e-9c1f-e2689f011088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds) # Calculates the size of each fold\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy)) # select a random index from dataset_copy\n",
    "            fold.append(dataset_copy.pop(index)) # remove from dataset_copy to avoid duplication.\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c2bda-e862-4ba8-956f-bd2283953f00",
   "metadata": {},
   "source": [
    "Comparing the predicted labels with the actual labels\n",
    "\n",
    "Initialize a variable to keep track of the number of correct predictions. For each data point, check if the actual label in the actual list (at index i) matches the predicted label in the predicted list (at index i). If they match, it means the prediction is correct, and ***correct*** is incremented by 1.\n",
    "\n",
    "After looping through all data points, calculate the accuracy percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "371dcf5f-cc30-4bc2-902a-4587ce8f75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0 # keep track of correct predictions\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]: #  check if the actual label matches the predicted label\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0 # get percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7ee97d-4791-4c2d-b26d-0e0bb6981ded",
   "metadata": {},
   "source": [
    "Call cross_validation_split with the dataset and n_folds arguments. For each fold, a training set and a test set are created. The training set is constructed by combining all the folds except the current fold. The line train_set = sum(train_set, []) is used to concatenate the lists representing each fold into a single training set list.\n",
    "\n",
    "A separate test set is created for each fold by copying the data points from the fold. Each row in the fold is copied to the test set, and the class label (the last element in each row) is set to None to simulate a scenario where the true labels are hidden during testing.\n",
    "\n",
    "The algorithm function is then called with the training set and test set for the current fold, we return predictions for the test set. The actual class labels for the test set are extracted from the current fold, and the accuracy_metric function is used to calculate the accuracy score by comparing the actual and predicted labels.\n",
    "\n",
    "The accuracy score is appended to the scores list for the current fold. After looping through all the folds, we return the list of evaluation scores, one for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1afad02e-2c9d-40c0-b970-3689cde0ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds) # create a training set by combining all folds except the current fold.\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, []) # separate the current fold as the test set.\n",
    "        test_set = list()\n",
    "        for row in fold: # preparing the test set by creating a copy of each row in the fold\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy) # Add copied row to the test_set\n",
    "            row_copy[-1] = None # remove the label from the row\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold] # Extract the actual labels from the current fold.\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b050a3-089d-4046-a2e9-5c196e969597",
   "metadata": {},
   "source": [
    "Here we define a function to make predictions with a set of weights. **weights[0]** is the bias term, while **weights[i + 1]** values correspond to the weights associated with the input features in **row**.\n",
    "\n",
    "We initialize **activation** with the value of **weights[0]**, then iterate over each feature in **row**. In each iteration **activation** is updated by adding the product of **weights[i + 1]** and **row[i]**. This is the weighted sum of the features, where each feature is multiplied by its corresponding weight.\n",
    "\n",
    "After exiting the loop, we check whether the final **activation** value is greater than or equal to **0.0**. If it is, return **1.0**, else, return **0.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d65021b-b410-4ad8-8d92-81c8bf16e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row) - 1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return 1.0 if activation >= 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb58ca2f-e1c5-4b01-bec9-757dcb449bad",
   "metadata": {},
   "source": [
    "#### Perceptron Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ba34b-18fc-4ff2-a5f0-02fc554ac8ed",
   "metadata": {},
   "source": [
    "**Gradient descent** is an optimization algorithm used to adjust the weights and biases of a machine learning model, such as a perceptron, in order to find the best set of parameters that minimize the loss. Gradient descent iteratively updates the parameters in a way that reduces the loss until it converges to a minimum.\n",
    "\n",
    "The loss function for a perceptron is based on the sum of errors for wrongly classified points, it's a piecewise linear function. \n",
    "\n",
    "> A piecewise linear function, is a mathematical function defined on a continuous range of real numbers, composed of straight-line segments, and each segment represents a linear function that includes both linear transformations and translations.\n",
    ">\n",
    "> In other words, a piecewise linear function is a way to represent complex functions by breaking them down into simpler linear segments, each defined over an interval.\n",
    "\n",
    "Gradient descent will update the perceptron's weights and bias in such a way that it tries to push the loss function towards its minimum, trying to correctly classify as many data points as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e41e3-c63e-48d8-b928-e7b3543ce61c",
   "metadata": {},
   "source": [
    "---\n",
    "The following function is for finding the optimal set of weights for the perceptron based on a training dataset.\n",
    "\n",
    "**l_rate** is the learning rate, which controls the step size in weight updates during the training process. It's considered a hyperparameter.\n",
    "\n",
    "> Hyperparameters are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning.\n",
    "\n",
    "An epoch is one complete pass through the entire training dataset. **n_epoch** is the number of training epochs. The length of the list **weights** is equal to the number of features in each data point.\n",
    "\n",
    "Iterate for *n* epochs. For each data point in the training dataset:\n",
    "- Make a prediction for the current data point **row** using the current set of weights.\n",
    "- The error is calculated by subtracting the predicted value from the actual target label.\n",
    "- The weight for the bias is updated by adding the product of the learning rate and the error. This gets the decision boundary closer to the correct classification.\n",
    "\n",
    "For each feature in the data point:\n",
    "- The weights for the individual features are updated similarly to account for each's contribution to the classification.\n",
    "\n",
    "Finally, the trained weights are returned as the result of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f55288-dcd4-4d2e-a26a-a22251a58a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "    weights = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            prediction = predict(row, weights)\n",
    "            error = row[-1] - prediction\n",
    "            weights[0] = weights[0] + l_rate * error\n",
    "            for i in range(len(row) - 1):\n",
    "                weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd4e2c8-d29c-4467-992c-c5148f8febb1",
   "metadata": {},
   "source": [
    "Defining the perceptron function.\n",
    "\n",
    "We store the predictions made by the Perceptron on the test data in an empty list **\"predictions\"**, then call the **train_weights** function to train the Perceptron on the provided training data. Return the weights learned during training.\n",
    "\n",
    "Iterate through the test dataset. For each data point in the test dataset:\n",
    "- Make a prediction for the current data point **row** using the weights learned during training.\n",
    "- The prediction for the current data point is added to the **predictions** list. This list contains the Perceptron's predictions *(0 or 1)* for each data point in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44b5065e-6e77-4231-acb4-f5066a4d91fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron Algorithm With Stochastic Gradient Descent\n",
    "def perceptron(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    weights = train_weights(train, l_rate, n_epoch)\n",
    "    for row in test:\n",
    "        prediction = predict(row, weights)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a222903-58f0-40ca-8425-0d742bd19cf6",
   "metadata": {},
   "source": [
    "#### Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f52fd-628d-4d38-aadb-d83288f83b16",
   "metadata": {},
   "source": [
    "We load and prepare the data from our CSV file, exclude the first row from the dataset using the **load_csv_exclude_first_row** function, to remove the header. Then convert the feature columns from strings to floats numbers using the **str_column_to_float** function per every row on the dataset. Finally, convert class labels from strings to unique integer values.\n",
    "\n",
    "We set **n_folds** to 3, for a 3-fold cross-validation. Set **l_rate** to 0.01 and **n_epoch** to 500 for the Perceptron.\n",
    "\n",
    "We then perform the cross-validation and model evaluation via **evaluate_algorithm**, passing the *perceptron* function for the ***algorithm*** parameter on the **evaluate_algorithm** function. The results of the model's performance on each fold are stored in the **scores** variable.\n",
    "\n",
    "Finally, print out the individual accuracy scores for each fold of the cross-validation, also calculate and print the mean accuracy across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c540d199-e51e-4899-af54-2048e32d06c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [76.81159420289855, 69.56521739130434, 72.46376811594203]\n",
      "Mean Accuracy: 72.947%\n"
     ]
    }
   ],
   "source": [
    "# Test the Perceptron algorithm on the sonar dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = './datasets/sonar.csv'\n",
    "dataset = load_csv_exclude_first_row(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# convert string class to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 3\n",
    "l_rate = 0.01\n",
    "n_epoch = 500\n",
    "scores = evaluate_algorithm(dataset, perceptron, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
